---
# hadoop/tasks/main.yml

# https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/SecureMode.html#User_Accounts_for_Hadoop_Daemons
- name: Create hadoop group
  group: name=hadoop

- name: Add hadoop users
  user:
    name: "{{ item }}"
    group: hadoop
    comment: "Hadoop {{ item }}"
    home: "{{ hadoop_home }}"
  with_items:
   - hdfs
   - yarn
   - mapred

- name: Create hadoop PID directory
  file:
    path: "{{ hadoop_pid_dir }}"
    state: directory
    group: hadoop
    mode: 0775

- name: Install software with apt
  apt:
    name: "{{ item }}"
    update_cache: yes
    cache_valid_time: 3600
    state: installed
  with_items: "{{ hadoop_apt_packages }}"

- name: Make dl_dir
  file:
    path: "{{ dl_dir }}"
    state: directory

- name: Download Hadoop
  get_url:
    url: "http://apache.mirrors.pair.com/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz"
    checksum: "sha256:49ad740f85d27fa39e744eb9e3b1d9442ae63d62720f0aabdae7aa9a718b03f7"
    dest: "{{ dl_dir }}/hadoop-2.7.2.tar.gz"
  register: hadoop_dl

- name: Make hadoop_home
  file:
    path: "{{ hadoop_home }}"
    state: directory
    group: hadoop

# ansible unarchive fails when using strip-components, known bug.
- name: Uncompress Hadoop
  when: hadoop_dl.changed
  command: "tar -xzf {{ hadoop_dl.dest }} -C {{ hadoop_home }} --strip-components=1"
  args:
    creates: "{{ hadoop_home }}/share"
  tags:
    - skip_ansible_lint #skip lint error due to strip-components bug

- name: Make Hadoop subdirs
  file:
    path: "{{ hadoop_home }}/{{ item }}"
    state: directory
    owner: root
    group: hadoop
    mode: 0775
  with_items:
    - logs
    - hdfs_data

- name: Download compiled H-Syndicate.jar from butler
  get_url:
    url: "https://butler.opencloud.cs.arizona.edu/jenkins/job/h-syndicate/lastSuccessfulBuild/artifact/dist/H-Syndicate.jar"
    dest: "{{ hdfs_lib_dir }}/H-Syndicate.jar"

- name: Download hadoop library dependencies
  get_url:
    url: "{{ item.url }}"
    checksum: "{{ item.cksum }}"
    dest: "{{ hdfs_lib_dir }}/{{ item.url | basename }}"
  with_items:
    - url: "http://central.maven.org/maven2/org/apache/commons/commons-collections4/4.0/commons-collections4-4.0.jar"
      cksum: "sha256:93f8dfcd20831a28d092427723f696bceb70b28e7fb89d7914f14d5ea492ce5a"
    - url: "http://central.maven.org/maven2/com/google/guava/guava/11.0.2/guava-11.0.2.jar"
      cksum: "sha256:e144a0ec7f5139c58d4f3729ccfb4240f9c576a1aa43790e4090e09316129ee1"
    - url: "http://central.maven.org/maven2/org/apache/httpcomponents/httpclient/4.1.1/httpclient-4.1.1.jar"
      cksum: "sha256:eae526d08a6679bf6ca138d45a0005b20ba6ec4a402788be810970713c8e4751"
    - url: "http://central.maven.org/maven2/org/apache/httpcomponents/httpcore/4.1.1/httpcore-4.1.1.jar"
      cksum: "sha256:4c3411ccc3b7dd6079b9c3721a059e610e69173a353efb6e049f93c338d0796d"
    - url: "http://central.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.5.2/jackson-jaxrs-1.5.2.jar"
      cksum: "sha256:5e4ea6a42da98f3c7256373578e96b081c48e094e7eae72a9d25662bcac3404f"
    - url: "http://central.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.5.2/jackson-xc-1.5.2.jar"
      cksum: "sha256:ba2b5de9fddbed33c67b0372c4f779fea11e61d2b50f6ff961daa5973461ec6b"
    - url: "http://central.maven.org/maven2/com/sun/jersey/contribs/jersey-apache-client4/1.8/jersey-apache-client4-1.8.jar"
      cksum: "sha256:a09205e3eac4ab3689ca01616af6664c6e74f0d6f4388efa62856c302584e98c"
    - url: "http://central.maven.org/maven2/com/sun/jersey/jersey-client/1.8/jersey-client-1.8.jar"
      cksum: "sha256:a663d591c5ae510c4b28435d6b5016eec7461d4fd0fb9011be1d977ef8a35dec"

- name: Template /etc/profile.d config files
  template:
    src: "{{ item }}.j2"
    dest: "/etc/profile.d/{{ item }}"
    owner: root
    group: root
    mode: 0755
  with_items:
    - hadoop.sh
    - java.sh

- name: Template hadoop configuration files
  template:
    src: "{{ item }}.j2"
    dest: "{{ hadoop_conf_dir }}/{{ item }}"
  with_items:
    - core-site.xml
    - hdfs-site.xml
    - mapred-site.xml
    - yarn-site.xml
    - yarn-env.sh
    - hadoop-env.sh
  notify:
    - restart-namenode
    - restart-datanode

- name: Copy over initscripts
  copy:
    src: "{{ item }}"
    dest: "/etc/init.d/{{ item }}"
    owner: "root"
    group: "root"
    mode: "0755"
  with_items:
    - hadoop-datanode
    - hadoop-namenode
    - hadoop-nodemanager
    - hadoop-resourcemanager

- name: check to see if HDFS has been formatted
  stat:
    path: "{{ hadoop_home }}/hdfs_data/name"
  register: hdfs_dn

- name: Format HDFS on namenode
  when: "inventory_hostname == groups['namenode'][0] and not hdfs_dn.stat.exists"
  shell: ". /etc/profile.d/java.sh && {{ hadoop_home }}/bin/hadoop namenode -format"

- name: Start hadoop processes on namenode
  when: "inventory_hostname == groups['namenode'][0]"
  service:
    name: "{{ item }}"
    state: started
  with_items:
    - hadoop-namenode
    - hadoop-resourcemanager

- name: Start hadoop processes on datanodes
  service:
    name: "{{ item }}"
    state: started
  with_items:
    - hadoop-datanode
    - hadoop-nodemanager
